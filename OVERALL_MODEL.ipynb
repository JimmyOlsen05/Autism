{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JimmyOlsen05/Autism/blob/main/OVERALL_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwLjPO6GipMg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Concatenate, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing import image\n",
        "!pip install tensorflow joblib tf2onnx\n",
        "\n",
        "import numpy as np\n",
        "!pip install dill\n",
        "import dill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqNMU_mvMh7H"
      },
      "source": [
        "BUILD AND TRAIN LSTM MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl5p9XNSNFo_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the M-CHAT dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Toddler Autism dataset July 2018.csv')\n",
        "\n",
        "# Drop the 'Qchat-10-Score' column as recommended\n",
        "data = data.drop('Qchat-10-Score', axis=1)\n",
        "\n",
        "# Encode the target variable ('Class/ASD Traits')\n",
        "label_encoder = LabelEncoder()\n",
        "data['Class/ASD Traits '] = label_encoder.fit_transform(data['Class/ASD Traits '])\n",
        "\n",
        "# Encode the categorical features\n",
        "categorical_cols = ['Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD', 'Who_completed_the_test']\n",
        "data = pd.get_dummies(data, columns=categorical_cols)\n",
        "\n",
        "# Convert boolean columns to integers\n",
        "bool_columns = ['Sex_f', 'Sex_m', 'Ethnicity_Hispanic', 'Ethnicity_Latino', 'Ethnicity_Native Indian', 'Ethnicity_Others', 'Ethnicity_Pacifica', 'Ethnicity_White European', 'Ethnicity_asian', 'Ethnicity_black', 'Ethnicity_middle eastern', 'Ethnicity_mixed', 'Ethnicity_south asian', 'Jaundice_no', 'Jaundice_yes', 'Family_mem_with_ASD_no', 'Family_mem_with_ASD_yes', 'Who_completed_the_test_Health Care Professional', 'Who_completed_the_test_Health care professional', 'Who_completed_the_test_Others', 'Who_completed_the_test_Self', 'Who_completed_the_test_family member']\n",
        "data[bool_columns] = data[bool_columns].astype(int)\n",
        "\n",
        "# Extract the sequence data and categorical features\n",
        "sequences = data[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10']].values.tolist()\n",
        "categorical_features = data.drop(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Class/ASD Traits '], axis=1)\n",
        "\n",
        "# Encode the sequences\n",
        "encoded_sequences = []\n",
        "for sequence in sequences:\n",
        "    encoded_seq = [int(value) for value in sequence]\n",
        "    encoded_sequences.append(encoded_seq)\n",
        "\n",
        "# Pad/truncate sequences to a fixed length\n",
        "max_sequence_length = max(len(seq) for seq in encoded_sequences)\n",
        "padded_sequences = pad_sequences(encoded_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = [padded_sequences, categorical_features]\n",
        "y = data['Class/ASD Traits ']\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_indices, test_indices = train_test_split(data.index, test_size=0.2, random_state=42)\n",
        "train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = [X[0][train_idx] for train_idx in train_indices]\n",
        "X_val = [X[0][val_idx] for val_idx in val_indices]\n",
        "X_test = [X[0][test_idx] for test_idx in test_indices]\n",
        "\n",
        "X_train_cat = X[1].iloc[train_indices]\n",
        "X_val_cat = X[1].iloc[val_indices]\n",
        "X_test_cat = X[1].iloc[test_indices]\n",
        "\n",
        "# Split the target variable into train, validation, and test sets\n",
        "y_train = y[train_indices]\n",
        "y_val = y[val_indices]\n",
        "y_test = y[test_indices]\n",
        "\n",
        "\n",
        "\n",
        "# Convert boolean columns to integers\n",
        "X_train_cat_int = X_train_cat.astype(int)\n",
        "X_val_cat_int = X_val_cat.astype(int)\n",
        "X_test_cat_int = X_test_cat.astype(int)\n",
        "\n",
        "# Reshape sequence data to include batch size and timestep dimensions\n",
        "X_train_seq_reshaped = np.expand_dims(X_train, axis=-1)\n",
        "X_val_seq_reshaped = np.expand_dims(X_val, axis=-1)\n",
        "X_test_seq_reshaped = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "# Define input layers for sequence data and categorical features\n",
        "sequence_input = Input(shape=(max_sequence_length, 1), name='sequence_input')\n",
        "cat_input = Input(shape=(X_train_cat_int.shape[1],), name='cat_input')\n",
        "\n",
        "# LSTM model for sequence data\n",
        "lstm_layer = LSTM(64)(sequence_input)\n",
        "\n",
        "# Concatenate LSTM output with categorical input\n",
        "concatenated = Concatenate()([lstm_layer, cat_input])\n",
        "\n",
        "# Dense layers\n",
        "dense1 = Dense(64, activation='relu')(concatenated)\n",
        "output = Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "# Define the model\n",
        "lstm_model = Model(inputs=[sequence_input, cat_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "lstm_model.summary()\n",
        "\n",
        "# Train the model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model_.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "history = lstm_model.fit([X_train_seq_reshaped, X_train_cat_int], y_train, epochs=10, batch_size=32,\n",
        "                    validation_data=([X_val_seq_reshaped, X_val_cat_int], y_val),\n",
        "                    callbacks=[early_stop, checkpoint])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = lstm_model.evaluate([X_test_seq_reshaped, X_test_cat_int], y_test)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "lstm_model.save('/content/drive/MyDrive/LSTM_model.h5')\n",
        "with open('/content/drive/MyDrive/autism_data.pkl', 'wb') as f:\n",
        "    dill.dump((encoded_sequences, categorical_cols, max_sequence_length,X_train_cat), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0zf_TXw8-Nh"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "lstm_model.save('/content/drive/MyDrive/LSTM_model.h5')\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from joblib import dump\n",
        "import tf2onnx\n",
        "import onnx\n",
        "import os\n",
        "\n",
        "# Assuming 'lstm_model' is your trained Keras LSTM model\n",
        "# lstm_model = ...\n",
        "\n",
        "# Define the paths for saving the model in different formats\n",
        "base_path = '/content/drive/MyDrive'\n",
        "h5_path = os.path.join(base_path, 'LSTM_model.h5')\n",
        "pickle_path = os.path.join(base_path, 'LSTM_model.pkl')\n",
        "joblib_path = os.path.join(base_path, 'LSTM_model.joblib')\n",
        "onnx_path = os.path.join(base_path, 'LSTM_model.onnx')\n",
        "saved_model_path = os.path.join(base_path, 'saved_model')\n",
        "json_path = os.path.join(base_path, 'LSTM_model.json')\n",
        "weights_path = os.path.join(base_path, 'LSTM_model_weights.h5')\n",
        "\n",
        "# Save as HDF5 (.h5)\n",
        "lstm_model.save(h5_path)\n",
        "print(f\"Model saved as HDF5 at {h5_path}\")\n",
        "\n",
        "# Save as Pickle (.pkl)\n",
        "with open(pickle_path, 'wb') as file:\n",
        "    pickle.dump(lstm_model, file)\n",
        "print(f\"Model saved as Pickle at {pickle_path}\")\n",
        "\n",
        "# Save as Joblib (.joblib)\n",
        "dump(lstm_model, joblib_path)\n",
        "print(f\"Model saved as Joblib at {joblib_path}\")\n",
        "\n",
        "# Save as ONNX (.onnx)\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(lstm_model)\n",
        "onnx.save(onnx_model, onnx_path)\n",
        "print(f\"Model saved as ONNX at {onnx_path}\")\n",
        "\n",
        "# Save as TensorFlow SavedModel\n",
        "lstm_model.save(saved_model_path)\n",
        "print(f\"Model saved as TensorFlow SavedModel at {saved_model_path}\")\n",
        "\n",
        "# Save model architecture as JSON and weights as HDF5\n",
        "model_json = lstm_model.to_json()\n",
        "with open(json_path, \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "lstm_model.save_weights(weights_path)\n",
        "print(f\"Model architecture saved as JSON at {json_path}\")\n",
        "print(f\"Model weights saved as HDF5 at {weights_path}\")\n",
        "\n",
        "# Note: PMML saving process is omitted here due to potential compatibility issues.\n",
        "# If your model is compatible, you can use the sklearn2pmml or other appropriate tools for conversion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lje4yO-fMY6I"
      },
      "source": [
        "BUILD AND TRAIN CNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOTQzhBgNokA"
      },
      "outputs": [],
      "source": [
        "# Define data paths (adjust according to your directory structure)\n",
        "train_data_dir = '/content/drive/MyDrive/NEW_DATASET_DIRECTORY/data/train'\n",
        "test_data_dir = '/content/drive/MyDrive/NEW_DATASET_DIRECTORY/data/test'\n",
        "validation_data_dir = '/content/drive/MyDrive/NEW_DATASET_DIRECTORY/data/valid'\n",
        "\n",
        "# Set image dimensions (modify if needed)\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# Data preprocessing and augmentation (consider these techniques)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=32,  # Adjust batch size as needed\n",
        "    class_mode='binary'  # Binary classification for autistic/non-autistic\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=32,  # Adjust batch size as needed\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Define CNN model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "# Model compilation\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Model training (adjust epochs and other hyperparameters as needed)\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=20,  # Adjust based on training time and validation performance\n",
        "                    validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr4gTcdn81PW"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/CNN_MODEL_.h5\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from joblib import dump\n",
        "import tf2onnx\n",
        "import onnx\n",
        "import os\n",
        "\n",
        "# Assuming 'model' is your trained Keras model\n",
        "# model = ...\n",
        "\n",
        "# Define the paths for saving the model in different formats\n",
        "base_path = '/content/drive/MyDrive'\n",
        "h5_path = os.path.join(base_path, 'CNN_MODEL_.h5')\n",
        "pickle_path = os.path.join(base_path, 'CNN_MODEL_.pkl')\n",
        "joblib_path = os.path.join(base_path, 'CNN_MODEL_.joblib')\n",
        "onnx_path = os.path.join(base_path, 'CNN_MODEL_.onnx')\n",
        "saved_model_path = os.path.join(base_path, 'saved_model')\n",
        "json_path = os.path.join(base_path, 'CNN_MODEL_.json')\n",
        "weights_path = os.path.join(base_path, 'CNN_MODEL_weights_.h5')\n",
        "\n",
        "# Save as HDF5 (.h5)\n",
        "model.save(h5_path)\n",
        "print(f\"Model saved as HDF5 at {h5_path}\")\n",
        "\n",
        "# Save as Pickle (.pkl)\n",
        "with open(pickle_path, 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "print(f\"Model saved as Pickle at {pickle_path}\")\n",
        "\n",
        "# Save as Joblib (.joblib)\n",
        "dump(model, joblib_path)\n",
        "print(f\"Model saved as Joblib at {joblib_path}\")\n",
        "\n",
        "# Save as ONNX (.onnx)\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
        "onnx.save(onnx_model, onnx_path)\n",
        "print(f\"Model saved as ONNX at {onnx_path}\")\n",
        "\n",
        "# Save as TensorFlow SavedModel\n",
        "model.save(saved_model_path)\n",
        "print(f\"Model saved as TensorFlow SavedModel at {saved_model_path}\")\n",
        "\n",
        "# Save model architecture as JSON and weights as HDF5\n",
        "model_json = model.to_json()\n",
        "with open(json_path, \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(weights_path)\n",
        "print(f\"Model architecture saved as JSON at {json_path}\")\n",
        "print(f\"Model weights saved as HDF5 at {weights_path}\")\n",
        "\n",
        "# Note: PMML saving process is omitted here due to potential compatibility issues.\n",
        "# If your model is compatible, you can use the sklearn2pmml or other appropriate tools for conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVx67COx9woV"
      },
      "outputs": [],
      "source": [
        "lstm_model=load_model(\"/content/drive/MyDrive/LSTM_model.h5\")\n",
        "#cnn_model=load_model(\"/content/drive/MyDrive/CNN_MODEL_.h5\")\n",
        "cnn_model=load_model(\"/content/drive/MyDrive/CNN_MODEL_.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F3jXg_hE6ZY"
      },
      "outputs": [],
      "source": [
        "'''import dill\n",
        "# Load the trained LSTM model\n",
        "lstm_model = load_model('/content/drive/MyDrive/LSTM_model.h5')\n",
        "\n",
        "\n",
        "# Load the trained CNN model\n",
        "cnn_model = load_model('/content/drive/MyDrive/CNN_MODEL_.json')\n",
        "import dill\n",
        "with open('/content/drive/MyDrive/autism_data.pkl', 'rb') as f:\n",
        "    encoded_sequences, categorical_cols, max_sequence_length,X_train_cat = dill.load(f)\n",
        "\n",
        "def predict_with_lstm(sequence, categorical_features):\n",
        "    padded_sequence = pad_sequences([sequence], maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "    padded_sequence_reshaped = np.expand_dims(padded_sequence, axis=-1)\n",
        "    categorical_features_int = np.array([categorical_features]).astype(int)\n",
        "\n",
        "    prediction = lstm_model.predict([padded_sequence_reshaped, categorical_features_int])\n",
        "    return prediction[0][0]\n",
        "\n",
        "img_width=150\n",
        "img_height=150\n",
        "\n",
        "def predict_with_cnn(image_path):\n",
        "    img = image.load_img(image_path, target_size=(img_width, img_height))\n",
        "    x = image.img_to_array(img)\n",
        "    x /= 255.0  # Rescale if necessary\n",
        "    x = np.expand_dims(x, axis=0)  # Add batch dimension\n",
        "    prediction = cnn_model.predict(x)\n",
        "    return prediction[0][0]\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "\n",
        "# Create input layers for the meta-model\n",
        "lstm_input = Input(shape=(1,), name='lstm_input')\n",
        "cnn_input = Input(shape=(1,), name='cnn_input')\n",
        "\n",
        "# Concatenate the inputs\n",
        "concatenated = Concatenate()([lstm_input, cnn_input])\n",
        "\n",
        "# Add a dense layer and output layer\n",
        "dense = Dense(64, activation='relu')(concatenated)\n",
        "output = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "# Create the meta-model\n",
        "meta_model = Model(inputs=[lstm_input, cnn_input], outputs=output)\n",
        "\n",
        "# Compile the meta-model\n",
        "meta_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the input for the LSTM model\n",
        "sequence = [1, 0, 1, 1, 0, 0, 0, 0, 1, 0]\n",
        "categorical_features = [0] * len(X_train_cat.columns)\n",
        "\n",
        "# Get the prediction from the LSTM model\n",
        "lstm_prediction = predict_with_lstm(sequence, categorical_features)\n",
        "\n",
        "# Prepare the input for the CNN model\n",
        "image_path = '/content/drive/MyDrive/DATASET/Non-Autistic-2.jpg'\n",
        "\n",
        "# Get the prediction from the CNN model\n",
        "cnn_prediction = predict_with_cnn(image_path)\n",
        "\n",
        "# Make a prediction using the ensemble model\n",
        "ensemble_prediction = meta_model.predict([np.array([lstm_prediction]), np.array([cnn_prediction])])\n",
        "\n",
        "# Convert the prediction to class label (0 or 1)\n",
        "predicted_class = 1 if ensemble_prediction > 0.5 else 0\n",
        "if predicted_class==1:\n",
        "  print(\"Autistic\")\n",
        "else:\n",
        "  print(\"Non-Autistic\")\n",
        "print(\"Predicted Class (Ensemble):\", predicted_class)\n",
        "meta_model.save('/content/drive/MyDrive/Meta_Model.save.h5')'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import dill\n",
        "from keras.models import load_model, model_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "\n",
        "# Load the trained LSTM model\n",
        "lstm_model = load_model('/content/drive/MyDrive/LSTM_model.h5')\n",
        "\n",
        "# Load the trained CNN model from JSON\n",
        "with open('/content/drive/MyDrive/CNN_MODEL_.json', 'r') as json_file:\n",
        "    cnn_model_json = json_file.read()\n",
        "cnn_model = model_from_json(cnn_model_json)\n",
        "cnn_model.load_weights('/content/drive/MyDrive/CNN_MODEL_weights_.h5')\n",
        "\n",
        "# Load the data\n",
        "with open('/content/drive/MyDrive/autism_data.pkl', 'rb') as f:\n",
        "    encoded_sequences, categorical_cols, max_sequence_length, X_train_cat = dill.load(f)\n",
        "\n",
        "# Define prediction functions\n",
        "def predict_with_lstm(sequence, categorical_features, lstm_model, max_sequence_length):\n",
        "    padded_sequence = pad_sequences([sequence], maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "    padded_sequence_reshaped = np.expand_dims(padded_sequence, axis=-1)\n",
        "    categorical_features_int = np.array([categorical_features]).astype(int)\n",
        "    prediction = lstm_model.predict([padded_sequence_reshaped, categorical_features_int])\n",
        "    return prediction[0][0]\n",
        "\n",
        "img_width = 150\n",
        "img_height = 150\n",
        "\n",
        "def predict_with_cnn(image_path, cnn_model):\n",
        "    img = image.load_img(image_path, target_size=(img_width, img_height))\n",
        "    x = image.img_to_array(img)\n",
        "    x /= 255.0  # Rescale if necessary\n",
        "    x = np.expand_dims(x, axis=0)  # Add batch dimension\n",
        "    prediction = cnn_model.predict(x)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Create and compile the meta-model\n",
        "lstm_input = Input(shape=(1,), name='lstm_input')\n",
        "cnn_input = Input(shape=(1,), name='cnn_input')\n",
        "concatenated = Concatenate()([lstm_input, cnn_input])\n",
        "dense = Dense(64, activation='relu')(concatenated)\n",
        "output = Dense(1, activation='sigmoid')(dense)\n",
        "meta_model = Model(inputs=[lstm_input, cnn_input], outputs=output)\n",
        "meta_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the input for the LSTM model\n",
        "sequence = [1, 0, 1, 1, 0, 0, 0, 0, 1, 0]\n",
        "categorical_features = [0] * len(X_train_cat.columns)\n",
        "\n",
        "# Get the prediction from the LSTM model\n",
        "lstm_prediction = predict_with_lstm(sequence, categorical_features, lstm_model, max_sequence_length)\n",
        "\n",
        "# Prepare the input for the CNN model\n",
        "image_path = '/content/drive/MyDrive/NEW_DATASET_DIRECTORY/data/train/Autistic/980.jpg'\n",
        "\n",
        "# Get the prediction from the CNN model\n",
        "cnn_prediction = predict_with_cnn(image_path, cnn_model)\n",
        "\n",
        "# Make a prediction using the ensemble model\n",
        "ensemble_prediction = meta_model.predict([np.array([lstm_prediction]), np.array([cnn_prediction])])\n",
        "\n",
        "# Convert the prediction to class label (0 or 1)\n",
        "predicted_class = 1 if ensemble_prediction > 0.5 else 0\n",
        "if predicted_class == 1:\n",
        "    print(\"Autistic\")\n",
        "else:\n",
        "    print(\"Non-Autistic\")\n",
        "\n",
        "print(\"Predicted Class (Ensemble):\", predicted_class)\n",
        "\n",
        "# Save the meta-model\n",
        "# meta_model.save('/content/drive/MyDrive/Meta_Model_.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbrapGf4qvKY",
        "outputId": "dd578495-5627-4530-cf7c-c2601db4f421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 394ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Non-Autistic\n",
            "Predicted Class (Ensemble): 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1IstjqHo7FPKgR9KA9NJlRM0n6lvESP8I",
      "authorship_tag": "ABX9TyPIgQm9SDGjyjgLBa2OB+Qy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}